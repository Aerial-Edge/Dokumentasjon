
\section{Blob detection.v1}

Since we use the Raspberry Pi Camera Module v3 and not a depth camera, we need to find a way to find the distance to a known object. We ended up using OpenCV and cvzone. Our approach involves detecting a specific color in the video frames, identifying the contours of the detected object, and then calculating the distance based on the object's apparent size in the image.

First, we import the necessary packages for image processing, numerical operations, and color detection:

\begin{lstlisting}[language=PythonPlus]
import cv2
import cvzone
from cvzone.ColorModule import ColorFinder
import numpy as np
\end{lstlisting}

We create a VideoCapture object (OpenCV) to read video frames from the camera \cite{OpenCVDoc}, and set the capture dimensions to 640x480 pixels \cite{OpenCVDoc}:

\begin{lstlisting}[language=PythonPlus]
cap = cv2.VideoCapture(0)
cap.set(3, 640)
cap.set(4, 480)
\end{lstlisting}

Next, we create a ColorFinder object (cvzone) with automatic color range update turned off \cite{CVzoneDoc}, and define the color range for detection (using predetermined HSV values):

\begin{lstlisting}[language=PythonPlus]
myColorFinder = ColorFinder(False)
hsvVals = {'hmin': 97, 'smin': 21, 'vmin': 23, 'hmax': 125, 'smax': 255, 'vmax': 193}
\end{lstlisting}

In a continuous loop, we read the video frames from the camera (OpenCV) \cite{OpenCVDoc}, detect the specified color range in the image using the ColorFinder object (cvzone) \cite{CVzoneDoc}, and find the contours in the binary mask (cvzone) \cite{CVzoneDoc}:

\begin{lstlisting}[language=PythonPlus]
while True:
    success, img = cap.read()
    imgColor, mask = myColorFinder.update(img, hsvVals)
    imgContour, contours = cvzone.findContours(img, mask)
\end{lstlisting}

When contours are detected, we extract data from the first contour (assumed to be the object of interest) and calculate the distance to the object based on its apparent size in the image. We use the real-world dimensions of the object (in this case, a tennis ball with a width of 6.5 cm) and the camera's focal length to compute the distance:

\begin{lstlisting}[language=PythonPlus]
if contours:
    data = contours[0]['center'][0], h - contours[0]['center'][1], int(contours[0]['area'])
    
    f = 535  # focal length of the camera
    W = 6.5  # real-world width of the tennis ball

    w = np.sqrt(contours[0]['area']/np.pi) * 2  # width of the tennis ball in the image
    d = (W * f) / w  # calculate the distance

    print(d)
\end{lstlisting}

Finally, we display the distance in centimeters on the image (cvzone) \cite{CVzoneDoc} and stack the original image, detected color image, binary mask, and contour image for visualization (cvzone) \cite{CVzoneDoc} this is done for testing purpose:

\begin{lstlisting}[language=PythonPlus]
cvzone.putTextRect(img, f'depth: {int(d)} cm', (contours[0]['center'][0] - 75, contours[0]['center'][1] - 50), scale= 2)
imgStack = cvzone.stackImages([img, imgColor, mask, imgContour], 2, 0.5)
cv2.imshow("Image", imgStack)
cv2.waitKey(1)
\end{lstlisting}

This approach is suitable for objects with known dimensions and relatively uniform color distribution. This testing was used with simple color detection. We can use this function to use the contours of trained models to find the distance as well. \newpage

\section{Blob detection.v2}

This code uses OpenCV \cite{OpenCVDoc} and cvzone \cite{CVzoneDoc} to track and measure the distance of 3 tennis-sized balls with different colors in a live video feed from a webcam.

First, we import the necessary packages:
\begin{lstlisting}[language=PythonPlus]
import cv2 as cv
from cvzone.FPS import FPS
import imutils
import math
import numpy as np
\end{lstlisting}

This function \verb|calculateDistance| is used to calculate the distance from the camera to the object based on the radius of the object in pixels. This calculation is based on similar triangles and the field of view of the camera.
\begin{lstlisting}[language=PythonPlus]
# Function to calculate distance from the object based on its radius in pixels
def calculateDistance(ballRadius_px):
    return int(faktor / ballRadius_px)
\end{lstlisting}

This function \verb|detect_colored_object| uses color-based filtering and shape-based detection to identify and locate the colored ball in the video frame \cite{ColoredObject}. The function uses color filtering to create a mask for pixels within the defined color range and applies the Hough Transform function \cite{OpenCVHoughCircle} on a blurred grayscale version of the frame to detect circles. If a detected circle's center lies within the color mask, the function returns the circle's coordinates and radius\cite{CircleDetection}.
\begin{lstlisting}[language=PythonPlus]
# Function to detect a colored object within a given color range and size
def detect_colored_object(colorLower, colorUpper, min_radius, max_radius):
    mask = cv.inRange(hsv, colorLower, colorUpper)
    
    # Erode the mask to remove noise & Dilate the mask to fill gaps
    mask = cv.erode(mask, None, iterations=2)
    mask = cv.dilate(mask, None, iterations=2)
    
    # Convert the frame to grayscale and apply median blur
    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    gray = cv.medianBlur(gray, 5)

    # Detect circles using the Hough transform
    circles = cv.HoughCircles(gray, cv.HOUGH_GRADIENT, 1, 20, param1=100, param2=30,
                              minRadius=min_radius, maxRadius=max_radius)

    # Check if any circles were detected
    if circles is not None:
        circles = np.uint16(np.around(circles))

        for circle in circles[0, :]:
            x, y, radius = circle

            # Check if the circle's center is within the mask's boundaries
            if 0 <= x < mask.shape[1] and 0 <= y < mask.shape[0] and mask[y, x] > 0:
                return (x, y, radius)
    return None
\end{lstlisting}

The function \verb|display_object_info| visualizes \cite{OpenCVDisplay} details such as a circle around the detected object, x, and y coordinates, and estimated distance on the video frame. 
\begin{lstlisting}[language=PythonPlus]
# Function to display information about the detected object on the frame
def display_object_info(frame, x, y, radius, distance, color, text_offset):
    if x is not None and y is not None:
        # Draw a circle around the detected object
        cv.circle(frame, (x, y), radius, color, 2)
        
        # Display the coordinates x and y
        coordinates_text = f"X: {x}, Y: {y}"
        
        # Display the distance text
        distance_text = f"Distance: {distance} cm"
        
        # Put the coordinate text on the frame
        cv.putText(frame, coordinates_text, (x + 10, y), cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        
        # Put the distance text on the frame
        cv.putText(frame, distance_text, (22, 70 + text_offset), cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
\end{lstlisting}

The \verb|ballRadius| is a variable indicating the radius of the tennis ball, the \verb|cameraFOV| is the camera's field of view, and lastly, the \verb|faktor| is a calculation parameter \cite{CircleParameters} used to determine the distance of the object from the camera. 
\begin{lstlisting}[language=PythonPlus]
ballRadius = 3.25   # cm (radius of the ball)
cameraFOV = 62.2    # degrees (field of view of the camera)
faktor = (1280 / 2) * (ballRadius / math.tan(math.radians(kameraFOV / 2))) # Pixels from center to edge divided by minimum distance from the lens
\end{lstlisting}

For each color, the program attempts to detect an object of that color in the frame \cite{OpenCVIterate}. If an object is found, its properties (coordinates, distance) are updated in the color dictionary. The updated information is then displayed \cite{OpenCVDisplay} on the frame. This process repeats for each frame, enabling real-time tracking.
\begin{lstlisting}[language=PythonPlus]
# Main loop
while True:
    (grabbed, frame) = videoCap.read() # Read a frame from the video capture
    fps, img = fpsreader.update(frame, color=(255, 0, 0)) # Update the FPS overlay on the frame
    frame = imutils.resize(frame, width=1280) # Resize the frame
    hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV) # Convert the frame to HSV format

    # Initialize x, y, and ballRadius_px values for each color
    for color_info in colors.values():
        color_info['x'] = None
        color_info['y'] = None
        color_info['ballRadius_px'] = None

    # Iterate through the defined colors and detect objects
    for idx, (color_name, color_info) in enumerate(colors.items()):
        # Call the detect_colored_object function to find objects in the frame
        obj = detect_colored_object(color_info['lower'], color_info['upper'], color_info['min_radius'],
                                    color_info['max_radius'])

        if obj: # If an object is detected, get the coordinates and radius, distance
            x, y, ballRadius_px = obj
            distance = kalkulerDistanse(ballRadius_px) 
            
            # Update the color_info dictionary with the new values
            color_info['x'] = x
            color_info['y'] = y
            color_info['ballRadius_px'] = ballRadius_px
            color_info['distance'] = distance
\end{lstlisting}

The \verb|display_object_info| function visually \cite{OpenCVDisplay} presents data about the detected object on the video feed.
\begin{lstlisting}[language=PythonPlus]
# Display information about the detected object on the frame
        display_object_info(frame, color_info['x'], color_info['y'], color_info['ballRadius_px'],
                            color_info.get('distance'), color_info['color'], color_info['text_offset'])

    cv.imshow("Frame", frame)  # Show the frame
\end{lstlisting}
Referring back to the discussion in the 'Configuration 4 Journey' section, it was clear that the computational demands of our program far exceeded the capabilities of the hardware in use, given that the development and testing were conducted on a high-end laptop utilizing a webcam. As a result, we needed to revisit our approach and develop Blob detection.v3. 

\section{Blob detection.v3}
By simplifying and
removing some of the more computationally intense functionalities from the program we developed Blob detection.v3.

First, we import the necessary packages:
\begin{lstlisting}[language=PythonPlus]
import cv2
import cvzone
from cvzone.ColorModule import ColorFinder
from cvzone.FPS import FPS
import numpy as np
\end{lstlisting}

The \verb|is_circle| function checks if a shape is circular enough to be considered a circle. If the shape is "round enough" (the roundness is more than the given threshold, which is set to 0.6), it filters out shapes that are not "round enough".
\begin{lstlisting}[language=PythonPlus]
# Function to check if a contour is a circle
def is_circle(cnt, threshold=0.6):
    area = cv2.contourArea(cnt) # Compute the area of the contour
    perimeter = cv2.arcLength(cnt, True) # Compute the perimeter of the contour
    if perimeter == 0: # If the perimeter is zero, it cannot be a circle, return False
        return False
    circularity = 4 * np.pi * area / (perimeter * perimeter) # Calculate the circularity of the contour
    return circularity >= threshold
\end{lstlisting}


Next, we create a \verb|ColorFinder| object (cvzone) with automatic color range update turned off \cite{CVzoneDoc}, and define the color range for detection using predetermined HSV (Hue, Saturation, Value) values:
\begin{lstlisting}[language=PythonPlus]
myColorFinder: ColorFinder = ColorFinder(False)

# Define the HSV color range for blue, green and orange
hsvValsBlue = {'hmin': 105, 'smin': 168, 'vmin': 119, 'hmax': 111, 'smax': 255, 'vmax': 255} #blue
hsvValsGreen = {'hmin': 76, 'smin': 29, 'vmin': 132, 'hmax': 97, 'smax': 124, 'vmax': 255} #green
hsvValsOrange = {'hmin': 0, 'smin': 120, 'vmin': 120, 'hmax': 20, 'smax': 255, 'vmax': 255} #orange
\end{lstlisting}

Each frame from the camera (OpenCV) \cite{OpenCVDoc}, is read and processed for frame rate, which at first is analyzed to find areas that match the color profiles defined for blue, green, and orange in HSVvalues\cite{CVzoneDoc}, and then analyze these colored areas further to identify contours in those regions. These contours are checked for circularity using the \verb|is_circle| function, thereby extracting contours that are likely to be circles.  
\begin{lstlisting}[language=PythonPlus]
    # Update the color detection for blue, green and orange
    imgColorBlue, mask = myColorFinder.update(img, hsvValsBlue)
    imgColorGreen, maskRed = myColorFinder.update(img, hsvValsGreen)
    imgColorOrange, maskOrange = myColorFinder.update(img, hsvValsOrange)

    # Find contours in blue, green and orange color mask 
    imgContourBlue, contours = cvzone.findContours(img, mask)
    imgContourGreen, contoursRed = cvzone.findContours(img, maskRed)
    imgContourOrange, contoursOrange = cvzone.findContours(img, maskOrange)

    # Filter contours that are circles
    circular_contours_blue = [cnt for cnt in contours if is_circle(cnt['cnt'])]
    circular_contours_green = [cnt for cnt in contoursRed if is_circle(cnt['cnt'])]
    circular_contours_orange = [cnt for cnt in contoursOrange if is_circle(cnt['cnt'])]
\end{lstlisting}

Processes and displays the depth, x-position, and y-position for each detected ball. It iterates over three lists containing circular contours for each color, and checks if there are circular contours detected. If so, it selects the first contour and calculates the depth of the ball using its area and predefined values for focal length and ball width \cite{distanceobject}. It then prints the color of the ball and its depth.
\begin{lstlisting}[language=PythonPlus]
# Process and display depth, x, and y position for each ball
    for color, circular_contours_list in zip(['blue', 'green', 'orange'],
                                             [circular_contours_blue, circular_contours_green, circular_contours_orange]):
        if circular_contours_list:
            cnt = circular_contours_list[0]
            data = cnt['center'][0], h - cnt['center'][1], int(cnt['area'])

            f = 474 #focal length of the camera
            W = 6.5 # real-world width of the tennis ball
            w = np.sqrt(cnt['area'] / np.pi) * 2 # width of the tennis ball in the image
            d = (W * f) / w # calculate the distance
            print(f"{color}: {d}")
\end{lstlisting}

The depth is displayed using \verb|cvzone.putTextRect| function\cite{CVzoneDoc} which adds the text "depth: {int(d)} cm" to the \verb|imgContourBlue| image. The x and y position is also displayed using \verb|cvzone.putTextRect|. \verb|ImgStack| is a stack of the original image, color image with a blue mask, binary mask, and contour image, which is then shown with \verb|cv2.imshow|\cite{CVzoneDoc}.
\begin{lstlisting}[language=PythonPlus]
            # Display depth on the frame
            cvzone.putTextRect(imgContourBlue, f'depth: {int(d)} cm',
                               (cnt['center'][0] - 75, cnt['center'][1] - 50), scale=2)
            # Display x and y position on the frame with more space between depth and position
            cvzone.putTextRect(imgContourBlue, f'x: {cnt["center"][0]}, y: {cnt["center"][1]}',
                               (cnt['center'][0] - 75, cnt['center'][1] - 10), scale=1.5)

    imgStack = cvzone.stackImages([img, imgColorBlue, mask, imgContourBlue], 2, 0.5)
    cv2.imshow("Image", imgStack)
\end{lstlisting}
This version offered a significant improvement in frame rate performance and detection and was further tested on Rasberry Pi. 

\newpage


\newpage
\section{Blob Detection.v3 Evaluation} \label{C4evaluation}

We first import the necessary packages to enable image processing, numerical operations, and color detection:

\begin{lstlisting}[language=PythonPlus]
import cv2
import cvzone
from cvzone.ColorModule import ColorFinder
import numpy as np
import xml.etree.ElementTree as ET
import os
import csv
\end{lstlisting}

The function \verb|is_circle| is created to check if a detected contour is circular or not. We calculate the area and the perimeter of the contour and use these values to calculate the circularity. A threshold is set to determine whether the contour is circular or not:

\begin{lstlisting}[language=PythonPlus]
def is_circle(cnt, threshold=0.6):
    area = cv2.contourArea(cnt)
    perimeter = cv2.arcLength(cnt, True)
    if perimeter == 0:
        return False
    circularity = 4 * np.pi * area / (perimeter * perimeter)
    return circularity >= threshold
\end{lstlisting}

The \verb|parse_label| function is designed to parse label information from an XML file. It extracts the name and bounding box coordinates of an object:

\begin{lstlisting}[language=PythonPlus]
def parse_label(xml_file):
    tree = ET.parse(xml_file)
    root = tree.getroot()

    label = {}
    for obj in root.findall('object'):
        name = obj.find('name').text
        box = obj.find('bndbox')
        xmin = int(box.find('xmin').text)
        xmax = int(box.find('xmax').text)
        ymin = int(box.find('ymin').text)
        ymax = int(box.find('ymax').text)
        
        label[name] = [(xmin, ymin, xmax, ymax)]
    return label
\end{lstlisting}
\newpage
We create the \verb|calculate_f1_score| function to compute the F1 score given precision and recall:\cite{preandrec}[p. 156]

\begin{lstlisting}[language=PythonPlus]
def calculate_f1_score(precision, recall):
    if precision + recall == 0:  # to avoid division by zero
        return 0
    else:
        f1_score = 2 * (precision * recall) / (precision + recall)
        return f1_score
\end{lstlisting}

Next, we define the \verb|read_image| function that simply reads an image from a given path using the OpenCV function \verb|imread|:

\begin{lstlisting}[language=PythonPlus]
def read_image(image_file):
    return cv2.imread(image_file)
\end{lstlisting}

In the \verb|detect_color| function, we detect colors in an image using a pre-defined HSV value range. We then use cvzone's findContours function to find the contours in the binary mask. A filter is applied to only select contours that pass the is\_circle test:

\begin{lstlisting}[language=PythonPlus]
def detect_color(image):
    myColorFinder = ColorFinder(False)
    #hsvVals = {'hmin': 49, 'smin': 69, 'vmin': 17, 'hmax': 108, 'smax': 255, 'vmax': 181} #green
    #hsvVals = {'hmin': 0, 'smin': 42, 'vmin': 0, 'hmax': 20, 'smax': 186, 'vmax': 219} #red
    hsvVals = {'hmin': 87, 'smin': 78, 'vmin': 0, 'hmax': 114, 'smax': 195, 'vmax': 174} #blue
    imgColor, mask = myColorFinder.update(image, hsvVals)
    imgContour, contours = cvzone.findContours(image, mask)

    circular_contours = [cnt for cnt in contours if is_circle(cnt['cnt'])]

    results = []
    if circular_contours:
        for cnt in circular_contours:
            x, y, w, h = cv2.boundingRect(cnt['cnt'])
            results.append(('blue', (x, y, x+w, y+h)))
    return results
\end{lstlisting}
\newpage
The \verb|calculate_iou| function is designed to compute the Intersection over Union (IoU) between two bounding boxes. This metric is commonly used in computer vision tasks to evaluate the accuracy of object detection models: \cite{IoUdoc}

\begin{lstlisting}[language=PythonPlus]
def calculate_iou(box1, box2):
    x1, y1, w1, h1 = box1
    x2, y2, w2, h2 = box2

    xi1 = max(x1, x2)
    yi1 = max(y1, y2)
    xi2 = min(x1 + w1, x2 + w2)
    yi2 = min(y1 + h1, y2 + h2)

    inter_area = max(xi2 - xi1, 0) * max(yi2 - yi1, 0)

    box1_area = w1 * h1
    box2_area = w2 * h2
    union_area = box1_area + box2_area - inter_area

    return inter_area / union_area if union_area > 0 else 0
\end{lstlisting}
\newpage
We define the \verb|calculate_precision_recall| function to compute precision and recall based on predictions and ground truth data. For each prediction, we calculate the maximum IoU with all ground truth boxes of the same color. If this maximum IoU is greater than or equal to a set IoU threshold, it's considered a true positive; otherwise, it's a false positive. False negatives are counted as those ground truth boxes that don't have any corresponding prediction with an IoU greater than or equal to the threshold:\cite{preandrec}[p. 155-156]

\begin{lstlisting}[language=PythonPlus]
def calculate_precision_recall(predictions, ground_truth, iou_threshold=0.5):
    TP = FP = FN = 0

    for image_predictions, image_ground_truth in zip(predictions, ground_truth):
        for pred_color, pred_box in image_predictions:
            if pred_color in image_ground_truth:
                ious = [calculate_iou(pred_box, truth_box) for truth_box in image_ground_truth[pred_color]]
                max_iou = max(ious) if ious else 0

                if max_iou >= iou_threshold:
                    TP += 1
                else:
                    FP += 1
            else:
                FP += 1

        for truth_color, truth_boxes in image_ground_truth.items():
            if truth_color not in [pred_color for pred_color, _ in image_predictions]:
                FN += len(truth_boxes)
            else:
                for truth_box in truth_boxes:
                    ious = [calculate_iou(pred_box, truth_box) for pred_color, pred_box in image_predictions if pred_color == truth_color]
                    max_iou = max(ious) if ious else 0
                    if max_iou < iou_threshold:
                        FN += 1

    precision = TP / (TP + FP) if TP + FP > 0 else 0
    recall = TP / (TP + FN) if TP + FN > 0 else 0

    return precision, recall
\end{lstlisting}
\newpage
In our main procedure, we define paths to our image and label directories, then sort and pair up the corresponding image and label files:

\begin{lstlisting}[language=PythonPlus]
image_dir = '/home/vaffe/RandomStuff/dataset/valid/blue/'
label_dir = '/home/vaffe/RandomStuff/dataset/valid/blue/labels/'

image_files = sorted(os.listdir(image_dir))
label_files = sorted(os.listdir(label_dir))
\end{lstlisting}

The code processes each image file and corresponding label file one by one. It reads the image, detects the color (our function returns the bounding boxes of detected objects), and parses the XML label file to obtain the ground truth bounding boxes. The predictions and ground truth are stored for later evaluation:

\begin{lstlisting}[language=PythonPlus]
for image_file, label_file in zip(image_files, label_files):
    image_path = os.path.join(image_dir, image_file)
    label_path = os.path.join(label_dir, label_file)

    image = read_image(image_path)
    label = parse_label(label_path)

    prediction = detect_color(image)

    predictions.append(prediction)
    ground_truths.append(label)

    print('Predicted: ', prediction)
    print('Ground Truth: ', label)

\end{lstlisting}

Finally, we calculate precision, recall, and F1 score, which are commonly used metrics to evaluate the performance of object detection models:

\begin{lstlisting}[language=PythonPlus]
precision, recall = calculate_precision_recall(predictions, ground_truths)
f1_score = calculate_f1_score(precision, recall)

print('Precision: ', precision)
print('Recall: ', recall)
print('F1 Score: ', f1_score)
\end{lstlisting}

The precision metric quantifies the number of correct positive predictions made, while recall (also known as sensitivity) quantifies the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 Score is the harmonic mean of precision and recall and provides a single score that balances both the concerns of precision and recall in one number. \cite{preandrec}[p. 155-156]


%References:

%[1] OpenCV VideoCapture. Retrieved from https://docs.opencv.org/master/d8/dfe/classcv_1_1VideoCapture.html

%[2] OpenCV VideoCapture::set. Retrieved from https://docs.opencv.org/master/d8/dfe/classcv_1_1VideoCapture.html#aa6480e6972ef4c00d74814ec841a2939

%[3] Cvzone. (02.22). ColorModule. Retrieved from  https://github.com/cvzone/cvzone/blob/master/cvzone/ColorModule.py

%[4] Cvzone. (02.22). findContours. Retrieved from https://github.com/cvzone/cvzone/blob/master/cvzone/Utils.py#L81

%[5] Cvzone. (02.22). putTextRect. Retrieved from https://github.com/cvzone/cvzone/blob/master/cvzone/Utils.py#L145

%[6] Cvzone. (02.22). stackImages. Retrieved from https://github.com/cvzone/cvzone/blob/master/cvzone/Utils.py#L12

\chapter{Config4 Source code}
\section{Blob Detection.v1}

\begin{lstlisting}[language=PythonPlus]
import cv2 
import cvzone 
from cvzone.ColorModule import ColorFinder
from cvzone.FPS import FPS  
import numpy as np 

fpsreader = FPS() 
cap = cv2.VideoCapture(0)  
cap.set(3, 640)  
cap.set(4, 480) 

success, img = cap.read()  
h, w, _ = img.shape  

myColorFinder = ColorFinder(False)  
hsvVals = {'hmin': 36, 'smin': 29, 'vmin': 44, 'hmax': 90, 'smax': 150, 'vmax': 187}

while True: 
    success, img = cap.read() 
    fps, img = fpsreader.update(img)
    imgColor, mask = myColorFinder.update(img, hsvVals)
    imgContour, contours = cvzone.findContours(img, mask)  

    if contours:  
        data = contours[0]['center'][0], h - contours[0]['center'][1], int(contours[0]['area'])

        f = 535
        W = 6.5
        w = np.sqrt(contours[0]['area'] / np.pi) * 2
        d = (W * f) / w

        x, y = contours[0]['center'][0], contours[0]['center'][1]

        print("x: ", x)
        print("y: ", y)

        print(fps)

        cvzone.putTextRect(img, f'depth: {int(d)} cm', (contours[0]['center'][0] - 75, contours[0]['center'][1] - 50),
                           scale=2)  
        cv2.putText(img, f'x: {int(x)}, y: {int(y)}', (20, h - 600), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 0),
                    thickness=2)

    imgStack = cvzone.stackImages([img, imgColor, mask, imgContour], 2,
                                  0.5)
    cv2.imshow("Image", imgStack) 

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()
\end{lstlisting}

\section{Blob Detection.v2}

\begin{lstlisting}[language=PythonPlus]
import cv2 as cv
from cvzone.FPS import FPS
import imutils
import math
import numpy as np

def calculateDistance(ballRadius_px):
    return int(faktor / ballRadius_px)

def detect_colored_object(colorLower, colorUpper, min_radius, max_radius):
    
    mask = cv.inRange(hsv, colorLower, colorUpper)
    mask = cv.erode(mask, None, iterations=2)
    mask = cv.dilate(mask, None, iterations=2)

    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)
    gray = cv.medianBlur(gray, 5)

    circles = cv.HoughCircles(gray, cv.HOUGH_GRADIENT, 1, 20, param1=100, param2=30,
                              minRadius=min_radius, maxRadius=max_radius)

    if circles is not None:
        circles = np.uint16(np.around(circles))

        for circle in circles[0, :]:
            x, y, radius = circle

            if 0 <= x < mask.shape[1] and 0 <= y < mask.shape[0] and mask[y, x] > 0:
                return (x, y, radius)
    return None

def display_object_info(frame, x, y, radius, distance, color, text_offset):
    if x is not None and y is not None:
       
        cv.circle(frame, (x, y), radius, color, 2)
        coordinates_text = f"X: {x}, Y: {y}"
        distance_text = f"Distance: {distance} cm"

        cv.putText(frame, coordinates_text, (x + 10, y), cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        cv.putText(frame, distance_text, (22, 70 + text_offset), cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

ballRadius = 3.25   # cm (radius of the ball)
cameraFOV = 62.2    # degrees (field of view of the camera)
faktor = (1280 / 2) * (ballRadius / math.tan(math.radians(kameraFOV / 2)))

colors = {
    'green': {
        'lower': (72, 70, 32), 
        'upper': (99, 244, 107),
        'min_radius': 0, #ex between 0 
        'max_radius': 0, #to ex 60 pixels
        'color': (0, 255, 0), 
        'text_offset': 0, #Distance text position under FPS
    },
    'orange': {
        'lower': (0, 115, 99), 
        'upper': (18, 255, 255), 
        'min_radius': 0,
        'max_radius': 0,
        'color': (0, 102, 255), 
        'text_offset': 20, 
    },
    'red': {
        'lower': (119, 37, 0), 
        'upper': (179, 179, 147), 
        'min_radius': 0,
        'max_radius': 0,
        'color': (0, 0, 255), 
        'text_offset': 40,  
    },
}

fpsreader = FPS() 
videoCap = cv.VideoCapture(0) 
videoCap.set(3, 1280)
videoCap.set(4, 720)

while True:
    (grabbed, frame) = videoCap.read() 
    fps, img = fpsreader.update(frame, color=(255, 0, 0)) 
    frame = imutils.resize(frame, width=1280) 
    hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV) 

    for color_info in colors.values():
        color_info['x'] = None
        color_info['y'] = None
        color_info['ballRadius_px'] = None

    for idx, (color_name, color_info) in enumerate(colors.items()):
        obj = detect_colored_object(color_info['lower'], color_info['upper'], color_info['min_radius'],
                                    color_info['max_radius'])
        if obj: 
            x, y, ballRadius_px = obj 
            distance = kalkulerDistanse(ballRadius_px) 
            
            color_info['x'] = x
            color_info['y'] = y
            color_info['ballRadius_px'] = ballRadius_px
            color_info['distance'] = distance

        display_object_info(frame, color_info['x'], color_info['y'], color_info['ballRadius_px'],
                            color_info.get('distance'), color_info['color'], color_info['text_offset'])

    cv.imshow("Frame", frame)  
    key = cv.waitKey(1)  
    if key == ord("q"):
        break
\end{lstlisting}

\section{Blob Detection.v3}

\begin{lstlisting}[language=PythonPlus]
import cv2
import cvzone
from cvzone.ColorModule import ColorFinder
from cvzone.FPS import FPS
import numpy as np

def is_circle(cnt, threshold=0.6):
    area = cv2.contourArea(cnt)
    perimeter = cv2.arcLength(cnt, True)
    if perimeter == 0:
        return False
    circularity = 4 * np.pi * area / (perimeter * perimeter)
    return circularity >= threshold

fpsreader = FPS()

cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

ret, frame = cap.read()
h, w, _ = frame.shape

myColorFinder: ColorFinder = ColorFinder(False)
hsvValsBlue = {'hmin': 105, 'smin': 168, 'vmin': 119, 'hmax': 111, 'smax': 255, 'vmax': 255} 
hsvValsGreen = {'hmin': 76, 'smin': 29, 'vmin': 132, 'hmax': 97, 'smax': 124, 'vmax': 255} 
hsvValsOrange = {'hmin': 0, 'smin': 120, 'vmin': 120, 'hmax': 20, 'smax': 255, 'vmax': 255} 

while True:
    ret, frame = cap.read()  
    fps, img = fpsreader.update(frame)  

    imgColorBlue, mask = myColorFinder.update(img, hsvValsBlue)  
    imgColorGreen, maskRed = myColorFinder.update(img, hsvValsGreen) 
    imgColorOrange, maskOrange = myColorFinder.update(img, hsvValsOrange)  

    imgContourBlue, contours = cvzone.findContours(img, mask)  
    imgContourGreen, contoursRed = cvzone.findContours(img, maskRed)  
    imgContourOrange, contoursOrange = cvzone.findContours(img, maskOrange)

    circular_contours_blue = [cnt for cnt in contours if is_circle(cnt['cnt'])]
    circular_contours_green = [cnt for cnt in contoursRed if is_circle(cnt['cnt'])]
    circular_contours_orange = [cnt for cnt in contoursOrange if is_circle(cnt['cnt'])]

    for color, circular_contours_list in zip(['blue', 'green', 'orange'],
                                             [circular_contours_blue, circular_contours_green, circular_contours_orange]):
        if circular_contours_list:
            cnt = circular_contours_list[0]
            data = cnt['center'][0], h - cnt['center'][1], int(cnt['area'])

            f = 474
            W = 6.5
            w = np.sqrt(cnt['area'] / np.pi) * 2
            d = (W * f) / w
            print(f"{color}: {d}")

            cvzone.putTextRect(imgContourBlue, f'depth: {int(d)} cm',
                               (cnt['center'][0] - 75, cnt['center'][1] - 50), scale=2)
            cvzone.putTextRect(imgContourBlue, f'x: {cnt["center"][0]}, y: {cnt["center"][1]}',
                               (cnt['center'][0] - 75, cnt['center'][1] - 10), scale=1.5)

    imgStack = cvzone.stackImages([img, imgColorBlue, mask, imgContourBlue], 2, 0.5)
    cv2.imshow("Image", imgStack)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()
\end{lstlisting}