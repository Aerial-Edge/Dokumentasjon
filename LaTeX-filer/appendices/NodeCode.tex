\section{Source code, ROS2 nodes}

\subsection{Image processing, configuration 1}

\subsubsection{Video capture node}

\begin{lstlisting}[language=PythonPlus, basicstyle=\tiny,]
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import cv2
import gi
gi.require_version("Gst", "1.0")
from gi.repository import Gst, GObject
import time

class ImagePublisher(Node):
    def __init__(self):
        super().__init__('image_publisher')
        self.publisher = self.create_publisher(Image, 'video_frames', 10)
        timer_period = 0.000166
        self.timer = self.create_timer(timer_period, self.timer_callback)
        Gst.init(None)
        # Configure a GStreamer pipeline to capture from a jetson.
        # nvarguscamerasrc sensor-id=0 select which camera sensor to capture from
        # video/x-raw(memory:NVMM) sets the output of the source to raw video stored in NVIDIA's proprietary NVMM format.
        # the width and the height is set to the expected input to the model
        # video/x-raw, format=BGRx converts the NVMM-formatted video to raw input
        # appsink drop=True parameter means that if the pipeline is running faster than it can handle, it will drop frames to maintain performance.
        self.pipeline = ("nvarguscamerasrc sensor-id=0 ! "
                                    "video/x-raw(memory:NVMM), width=640, height=640, framerate=30/1, format=NV12 ! "
                                    "nvvidconv flip-method=0 ! "
                                    "video/x-raw, width=640, height=640, format=BGRx ! "
                                    "videoconvert ! "
                                    "video/x-raw, format=BGR ! appsink drop=True")
        self.cap = cv2.VideoCapture(self.pipeline, cv2.CAP_GSTREAMER)
        self.br = CvBridge()
        self.timer_start = time.time()
        self.timer_end = time.time()
        self.period = 0
        self.fps = 0

    def timer_callback(self):
        ret, frame = self.cap.read()
        self.timer_end = time.time()
        self.period = self.timer_end - self.timer_start
        self.fps = 1/self.period
        self.timer_start = time.time()
        # CV bridge wraps the image array in ROS image message
        self.publisher.publish(self.br.cv2_to_imgmsg(frame))

        self.get_logger().info('Video frame published')





def main(args=None):
    # Initialize ROS client library
    rclpy.init(args=args)
    image_publisher = ImagePublisher()
    rclpy.spin(image_publisher)
    image_publisher.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
\end{lstlisting}

\subsubsection{Object detection node}

\begin{lstlisting}[language=PythonPlus, basicstyle=\tiny,]
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import Int32MultiArray
from cv_bridge import CvBridge
import cv2
import numpy as np
import time
from .yoloDet import YoloTRT


# Absolute path to used library and tensorRT model
model = YoloTRT(library="/home/gruppe6/ros2_ws/src/config1/config1/yolov5/build/libmyplugins.so", 
                engine="/home/gruppe6/ros2_ws/src/config1/config1/yolov5/build/best2.engine", conf=0.5, yolo_ver="v5")



class Detect(Node):
    def __init__(self):
        # Instantiate parent class object (Node)
        super().__init__('detect')
        # Create subscription to video_frames topic
        self.subscription = self.create_subscription(Image, 'video_frames', self.listener_callback, 10)
        # Create publisher to object_pos_and_distance topic
        self.publisher = self.create_publisher(Int32MultiArray, 'object_pos_and_distance', 10)
        # cv_bridge i s used to extract the image array from the ROS image msg
        self.br = CvBridge()

        self.frame_count = 0
        self.period_timer_start = time.time()
        self.period_timer_end = time.time()
        self.fps = 0
        

        
    # Callback for video_frames subscription , called every
    # time a new video frame i s recieved
    def listener_callback(self, data):
        self.frame_count += 1
        self.period_timer_end = time.time()
        self.timer_period = self.period_timer_end - self.period_timer_start
        self.fps = 1 / self.timer_period
        self.period_timer_start = time.time()
        self.get_logger().info('Recieving video frame, current FPS: {:.2f}'.format(self.fps))
        current_frame = self.br.imgmsg_to_cv2(data)
        detections, t = model.Inference(current_frame)
        for obj in detections:
            # print(obj['class'], obj['conf'], obj['center'], obj['width'])
            cx = int(obj['center'][0])
            cy = int(obj['center'][1])
            f = 434
            W = 6.5
            w = obj['width']
            distance = int((W*f)/w)
            
            # creating a ros2 int32multiarray
            msg = Int32MultiArray()
            msg.data = [cx, cy, distance]
            self.publisher.publish(msg)



def main(args=None):
    # Initialize ROS client library
    rclpy.init(args=args)
    # Instantiate the detect node
    detect = Detect()
    # Spin node forever
    rclpy.spin(detect)
    
    detect.destroy_node()
    rclpy.shutdown()
        

if (__name__ == "__main__"):
    main()

\end{lstlisting}


\subsection{Image processing, configuration 2/3}

\subsubsection{Video capture node}

\begin{lstlisting}[language=PythonPlus, basicstyle=\tiny,]
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import cv2
from picamera2 import Picamera2

width = 384
height = width

class VideoCapture(Node):
    def __init__(self):
        super().__init__('video_capture')
        self.publisher = self.create_publisher(Image, 'video_frames', 10)
        # Timer callback is triggered 60 times per second
        timer_period = 1/60
        self.timer = self.create_timer(timer_period, self.timer_callback)
        self.picam2 = Picamera2()
        # Configure picamera2 to capture video with the format and size expected by the model.
        # This way we avoid having to reformat and resize later
        # raw is set to 1640x1232 to force high FOV sensor mode.
        # FrameDurationLimits is the min and max frame period in microseconds, 40000us limits capture
        # to 25 FPS
        self.picam2.configure(self.picam2.create_video_configuration(main={"format": 'RGB888', "size": (width, height)},
                                                    raw={"size": (1640,1232)},
                                                    controls={"FrameDurationLimits": (40000, 40000)}))
        self.picam2.start()
        self.br = CvBridge()

    def timer_callback(self):
        frame = self.picam2.capture_array()

        # Cv bridge wraps the image array in a ROS image message
        self.publisher.publish(self.br.cv2_to_imgmsg(frame))
        self.get_logger().info('Video frame published')


def main(args=None):
    # Initialize ROS client library
    rclpy.init(args=args)

    # Instantiate the video_capture node
    video_capture = VideoCapture()
    
    # Spin node forever
    rclpy.spin(video_capture)
    video_capture.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()


\end{lstlisting}


\subsubsection{Object detection node}

\begin{lstlisting}[language=PythonPlus, basicstyle=\tiny,]

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import Int32MultiArray
from cv_bridge import CvBridge
import cv2
import numpy as np
import tflite_runtime.interpreter as tflite
import time
from picamera2 import Picamera2



# Absolute path to tflite model
model ='/home/gruppe6/models/edl1_1k_edgetpu.tflite'

# Load edgetpu runtime shared library
tpu_interpreter = tflite.Interpreter(model, experimental_delegates=[
    tflite.load_delegate('libedgetpu.so.1.0')])

cpu_interpreter = tflite.Interpreter(model)
# We discard detections with lower confidence score than the threshold
threshold = 0.80

class Detect(Node):
    def __init__(self):
        # Instantiate parent class object (Node)
        super().__init__('detect')

        # Create subscription to video_frames topic
        self.subscription = self.create_subscription(Image, 'video_frames', self.listener_callback, 10)

        # Create publisher to object_pos_and_distance topic
        self.publisher = self.create_publisher(Int32MultiArray, 'object_pos_and_distance', 10)

        # cv_bridge is used to extract the image array from the ROS image msg
        self.br = CvBridge()

        # Selected interpreter, change to cpu_interpreter to run
        # model on cpu only
        self.interpreter = tpu_interpreter

        # Allocate tensors, must be called to start inference
        self.interpreter.allocate_tensors()

        # Input details for the loaded model
        self.input_details = self.interpreter.get_input_details()

        # Output details for the loaded model
        self.output_details = self.interpreter.get_output_details()

        self.period_timer_start = time.time()
        self.period_timer_end = time.time()
        self.fps = 0
        # Multiplier used for distance calculation
        self.focal_length_multiplier = 847 # pi camera 2

        self.ball_real_diameter = 6.5 # cm
        # ROS2 message type, used to publish to 'object pos and distance'
        self.output_array = Int32MultiArray()

        # Get image dimensions expected by the model
        self.width = self.input_details[0]['shape'][1]
        self.height = self.input_details[0]['shape'][2]
        
    # Returns False if val is outside cutoff, used to discard
    # distance measurements at the edges of the frame
    def constrain_detection(self, val, frame_dim, cutoff):
        return val > cutoff and val < (frame_dim - cutoff)
    
    # Callback for video_frames subscription, called every 
    # time a new video frame is recieved
    def listener_callback(self, data):
        self.period_timer_end = time.time()
        self.timer_period = self.period_timer_end - self.period_timer_start
        self.fps = 1 / self.timer_period
        self.period_timer_start = time.time()
        self.get_logger().info('Current FPS: {:.2f}'.format(self.fps))
        current_frame = self.br.imgmsg_to_cv2(data)

        
        # Add batch dimension expected by the model,
        # new shape = [1, WIDTH, HEIGHT, 3]
        input_data = np.expand_dims(current_frame, axis=0)

        # Set input_data as the model input
        self.interpreter.set_tensor(self.input_details[0]['index'],
                                    input_data)
        
        # Start inference by invoking the interpreter
        self.interpreter.invoke()

        # boxes, classes, scores and number of detections are the
        # available model outputs
        # We are only using boxes and scores in this node
        
        # output_details returns the tensor index needed by get_tensor()
        boxes = self.interpreter.get_tensor(self.output_details[1]['index'])[0]
        scores = self.interpreter.get_tensor(self.output_details[0]['index'])[0]




        # Loop over all detections
        for i in range(len(scores)):
            if ((scores[i] > threshold) and (scores[i] <= 1.0)):
                # Get corner coordinates of bounding box
                x1, x2 = int(boxes[i][1] * self.width) , int(boxes[i][3] * self.width)
                y1, y2 = int(boxes[i][0] * self.height), int(boxes[i][2] * self.height)

                w, h = x2 - x1, y2 - y1
                # Get center of bounding box
                cx, cy = (int(x1 + 0.5*w),int(y1+0.5*h))

                # Only send info on the best score
                if (scores[i] > best_score):
                    best_score = scores[i]
                    cx_out = cx
                    cy_out = cy
                    distance_out = int((self.dist_multiplier)/w)

                # Because distance calculation is based on width, we need to discard
                # detections at the left and right edges as the width may be cropped
                if (self.constrain_detection(cx, self.width, 30)):
                    dist = int((self.ball_real_diameter * self.focal_length_multiplier) / w)
                    self.output_array.data = [cx_out, cy_out, distance_out]
                    self.publisher.publish(self.output_array)
                else:
                    self.output_array.data = [cx_out, cy_out, -1]
                    self.publisher.publish(self.output_array)
            # No detections
            else:
                self.output_array.data = [-1, -1, -1]
                self.publisher.publish(self.output_array)



def main(args=None):
    # Initialize ROS client library
    rclpy.init(args=args)

    # Instantiate the detect node
    detect = Detect()

    # Spin node forever
    rclpy.spin(detect)

    detect.destroy_node()
    rclpy.shutdown()
    

if (__name__ == "__main__"):
    main()


\end{lstlisting}






\subsection{Object follower}

\subsubsection{Controller node}

\begin{lstlisting}[language=PythonPlus, basicstyle=\tiny,]

import rclpy
from rclpy.node import Node
from std_msgs.msg import Int32MultiArray
from .object_follower import ObjectFollower



class Controller(Node):
    def __init__(self):
        super().__init__('controller')
        self.subscription = self.create_subscription(Int32MultiArray, 'object_pos_and_distance', self.listener_callback, 10)
        self.publisher = self.create_publisher(Int32MultiArray, 'yaw_thrust_pitch', 10)
        # Instantiate an ObjectFollower object
        # PIDs must be set up by editing the constructor in object_follower.py
        self.object_follower = ObjectFollower()

    # Callback for the 'object_pos_and_distance' subscription
    def listener_callback(self, msg_in):
        # Feed values from 'object_pos_and_distance' to the object follower
        self.object_follower(x=msg_in.data[0], y=msg_in.data[1], distance=msg_in.data[2])
        msg_out = Int32MultiArray()
        # Publish object follower outputs to 'yaw_thrust_pitch' topic
        msg_out.data = [self.object_follower.yaw_out, self.object_follower.thrust_out, self.object_follower.pitch_out ]
        self.publisher.publish(msg_out)


def main(args=None):
    rclpy.init(args=args)
    controller = Controller()
    rclpy.spin(controller)
    controller.destroy_node()
    rclpy.shutdown()
    

if (__name__ == "__main__"):
    main()


\end{lstlisting}

\subsubsection{Object follower class}

\begin{lstlisting}[language=PythonPlus, basicstyle=\tiny,]

from .pid import PID




class ObjectFollower:
    # Setpoints for yaw and thrust PIDs are 160 to keep detected
    # object in the center of a 320x320 frame.
    # Pitch setpoint of 70 to keep detected object at 70 cm distance
    def __init__(self):
        self.yaw_pid = PID(160, 0.5, 0.0, 0.0, -1000, 1000)
        self.thrust_pid = PID(160, 0.5, 0.0, 0.0, 300, 700)
        self.pitch_pid = PID(70, 0.5, 0.0, 0.0, -1000, 1000)
        self.yaw_out = 0
        self.thrust_out = 0
        self.pitch_out = 0

    def __call__(self, x, y, distance):
        if (x != -1):
            self.yaw_out = int(self.yaw_pid(x))
            self.thrust_out = int(self.thrust_pid(y))
            self.pitch_out = int(self.pitch_pid(distance))
        # if x = -1 no object is being detected, drone should
        # move slowly towards ground with 300 thrust (500 is neutral thrust)
        else:
            self.yaw_out = 0
            self.thrust_out = 300
            self.pitch_out = 0

    def tune_yaw(self, kp, ki, kd):
        self.yaw_pid.tune(kp, ki, kd)

    def tune_thrust(self, kp, ki, kd):
        self.thrust_pid.tune(kp, ki, kd)

    def tune_pitch(self, kp, ki, kd):
        self.pitch_pid.tune(kp, ki, kd)

\end{lstlisting}

\subsubsection{PID class}

\begin{lstlisting}[language=PythonPlus, basicstyle=\tiny,]

import time




def clamp(val: float, lower_limit: float, upper_limit: float):
    return lower_limit if val <= lower_limit else upper_limit if val > upper_limit else val

class PID:

    def __init__(self,
                 setpoint: float = 0.0,
                 kp: float = 0.0,
                 ki: float = 0.0,
                 kd: float = 0.0,
                 min_output: float = None,
                 max_output: float = None,
                 dt_min: float = 0.01):
        

        # setpoint: PID controller setpoint
        # kp: proportional gain constant
        # ki: integral gain constant
        # kd: derivative gain constant
        # dt_min: minimum time between error corrections

        self.setpoint = setpoint
        self.kp, self.ki, self.kd = kp, ki, kd
        self.dt_min = dt_min
        self.min_output = min_output
        self.max_output = max_output
        
        self.p_out: float = 0.0
        self.i_out: float = 0.0
        self.d_out: float = 0.0
        self.prev_time = time.time()
        self.prev_input: float = 0.0
        self.prev_output: float = 0.0


    def __call__(self, input):
        current_time = time.time()
        dt = current_time - self.prev_time
        self.prev_time = current_time
        d_input = input - self.prev_input

        if dt < self.dt_min:
            return self.prev_output
        
        error = self.setpoint - input

        # proportional control signal
        self.p_out = self.kp * error

        # integral control signal
        self.i_out += self.ki * error * dt
        self.i_out = clamp(self.i_out, self.min_output, self.max_output)

        # derivative control signal
        self.d_out = self.kd * d_input / dt

        output = self.p_out + self. i_out + self.d_out
        output = clamp(output, self.min_output, self.max_output)

        self.prev_input = input
        self.prev_output = output

        return output
    
    def tune(self, kp, ki, kd):
        self.kp = kp
        self.ki = ki
        self.kd = kd

    def get_kp(self):
        return self.kp
    
    def get_ki(self):
        return self.ki
    
    def get_kd(self):
        return self.kd


\end{lstlisting}

\subsubsection{Autopilot}
\begin{lstlisting}[language=PythonPlus, basicstyle=\tiny,]
import rclpy
from rclpy.node import Node
from std_msgs.msg import Int32MultiArray
from pymavlink import mavutil
import socket

# When simulating the flight controller firmware through SITL,
# MAVLink connection is transmitted over UDP instead of UART / USB

# Quickly creates a socket to get programmatically get ahold of current IP-address:
def get_ip_address():
    = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    s.connect(("192.168.1.1", 80))
    return s.getsockname()[0]

class Autopilot(Node):
    def __init__(self):
        super().__init__('autopilot')
        self.get_logger().info('Node started')
        # Subscribes to yaw, thrust and pitch values being computed from the PIDs in "Controller node":
        self.subscription = self.create_subscription(Int32MultiArray, 'yaw_thrust_pitch', self.listener_callback, 10)
        self.serial0_udp = 'udpin:' + get_ip_address() + ':14550'
        # Tries to establish MAVLink connection on udpin:[IP_ADDRESS]:14550 and waits for heartbeat:
        self.the_connection = mavutil.mavlink_connection(self.serial0_udp)  
        self.the_connection.wait_heartbeat()
        self.get_logger().info('Heartbeat from: %s' % self.the_connection.target_system)

    def listener_callback(self, msg_in):
        self.the_connection.mav.manual_control_send(    self.the_connection.target_system,  # Established after heartbeat
                                                        msg_in.data[2],                     # Established after heartbeat
                                                        0,                                  # Roll value (static)
                                                        msg_in.data[1],                     # Thrust value
                                                        msg_in.data[0],                     # Yaw value
                                                        0)                                  # Bitfield corresponding to extra
                                                                                            # buttons , not needed and can be
                                                                                            # set to 0 for this purpose

def main(args=None):
    rclpy.init(args=args)
    autopilot = Autopilot()
    rclpy.spin(autopilot)
    autopilot.destroy_node()
    rclpy.shutdown()


if (__name__ == "__main__"):
    main()

\end{lstlisting}

\newpage
\subsection{Image Processing, configuration 4}\label{ros2nodeC4}

\subsubsection{Video capture node}

This script creates a ROS2 node capable of capturing and streaming video data from a Raspberry Pi Camera using ROS2, OpenCV, cv\_bridge, and picamera2. 

First, we import the required packages that allow us to establish ROS2 nodes, manage ROS2 image data, work with the Raspberry Pi camera, and convert between ROS2 and OpenCV image formats:

\begin{lstlisting}[language=PythonPlus]
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
import cv2
from cv_bridge import CvBridge
from picamera2 import Picamera2
from libcamera import controls
\end{lstlisting}

Then we define the \verb|CameraCapture| class, inheriting from Node. In this class's constructor, we initialize the parent class with the name \verb|camera_capture|, generate a publisher to transmit Image messages over the \verb|image_data| topic, establish a timer with a callback function for publishing image data, configure and initiate the Raspberry Pi camera, and instantiate a bridge for converting between OpenCV and ROS image formats:

\begin{lstlisting}[language=PythonPlus]
class CameraCapture(Node):
    def init(self):
        super().init('camera_capture')
        self.publisher_ = self.create_publisher(Image, 'image_data', 10)
        self.timer = self.create_timer(1/30, self.publish_image_data)
        self.opencv_video = Picamera2()
        self.opencv_video.configure(self.opencv_video.create_preview_configuration(main={"format": 'RGB888', "size": (640, 480)}))
        self.opencv_video.set_controls({"AwbEnable": True})
        self.opencv_video.start()
        self.bridge = CvBridge()
\end{lstlisting}

In the \verb|publish_image_data| function, we capture a frame from the Raspberry Pi camera, convert it to a ROS2 message, and publish the message:

\begin{lstlisting}[language=PythonPlus]
def publish_image_data(self):
    frame = self.opencv_video.capture_array()
    msg = self.bridge.cv2_to_imgmsg(frame, encoding='bgr8')
    self.publisher_.publish(msg)
\end{lstlisting}

The main function initializes the ROS2 client library (rclpy), creates an instance of our \verb|CameraCapture| node, waits for incoming messages, properly destroys the node when done, and finally shuts down the ROS2 client library:

\begin{lstlisting}[language=PythonPlus]
def main(args=None):
    rclpy.init(args=args)
    node = CameraCapture()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if name == 'main':
    main()
\end{lstlisting}

\newpage
\subsubsection{Blob Detection}

This task involves developing a ROS2 node capable of detecting specific color objects in a video stream, estimating their relative positions and distances, and publishing this information using ROS2. The utilized packages include ROS2 (rclpy), OpenCV, cv\_bridge, cvzone, and numpy.

First, we import the necessary packages for establishing ROS2 nodes, handling ROS2 messages, image processing, blob detection and numerical operations:

\begin{lstlisting}[language=PythonPlus]
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import Int32MultiArray
import cv2
from cv_bridge import CvBridge
import cvzone
from cvzone.FPS import FPS
from cvzone.ColorModule import ColorFinder
import numpy as np
import time
import math
\end{lstlisting}

We define the \verb|ObjectDetection| class, which inherits from \verb|Node|. In the constructor of this class, we create a subscriber to receive Image messages from the \verb|image_data| topic, initialize the FPS reader, bridge for converting between OpenCV and ROS2 image formats, and a publisher to broadcast \verb|Int32MultiArray| messages (the object position and distance data) on the \verb|distance_and_pos| topic:

\begin{lstlisting}[language=PythonPlus]
class ObjectDetection(Node):
    def init(self):
        super().init('object_detection')
        self.subscription = self.create_subscription(Image, 'image_data', self.process_image, 10)
        self.bridge = CvBridge()
        self.fpsreader = FPS()
        self.distance_and_position_publisher = self.create_publisher(Int32MultiArray, 'distance_and_pos', 10)
\end{lstlisting}

The \verb|is_circle| function is defined to assess the circularity of a contour based on its area and perimeter:

\begin{lstlisting}[language=PythonPlus]
def is_circle(self, cnt, threshold=0.75):
    area = cv2.contourArea(cnt)
    perimeter = cv2.arcLength(cnt, True)
    if perimeter == 0:
        return False
    circularity = 4 * np.pi * area / (perimeter * perimeter)
    return circularity >= threshold
\end{lstlisting}

In the \verb|process_image| function, we convert the incoming ROS message to an OpenCV image, detect specific colors (blue, green, red) in the image, filter the resulting contours for circularity, compute the distance and position of each detected object based on the size and location of the contours, log this information, and publish it as a Int32MultiArray message. If you want to read more about how the detection algorithm works read \ref{algo}.

\begin{lstlisting}[language=PythonPlus]
def process_image(self, msg):

    opencv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
    h, w, _ = opencv_image.shape

    myColorFinder: ColorFinder = ColorFinder(False)
    hsvValsBlue = {'hmin': 104, 'smin': 128, 'vmin': 0, 'hmax': 120, 'smax': 255, 'vmax': 152}
    hsvValsGreen = {'hmin': 26, 'smin': 54, 'vmin': 48, 'hmax': 90, 'smax': 137, 'vmax': 255}
    hsvValsRed = {'hmin': 0, 'smin': 120, 'vmin': 120, 'hmax': 20, 'smax': 255, 'vmax': 255}

    fps, img = self.fpsreader.update(opencv_image)

    _, maskBlue = myColorFinder.update(img, hsvValsBlue)
    _, maskGreen = myColorFinder.update(img, hsvValsGreen)
    _, maskRed = myColorFinder.update(img, hsvValsRed)

    _, contoursBlue = cvzone.findContours(img, maskBlue)
    _, contoursGreen = cvzone.findContours(img, maskGreen)
    _, contoursRed = cvzone.findContours(img, maskRed)

    circular_contours_blue = [cnt for cnt in contoursBlue if self.is_circle(cnt['cnt'])]
    circular_contours_green = [cnt for cnt in contoursGreen if self.is_circle(cnt['cnt'])]
    circular_contours_red = [cnt for cnt in contoursRed if self.is_circle(cnt['cnt'])]

    for color, circular_contours_list in zip(['blue', 'green', 'red'],
                                            [circular_contours_blue, circular_contours_green, circular_contours_red]):
        if circular_contours_list:

            cnt = circular_contours_list[0]
            x, y = cnt['center']

            f = 889
            W = 6.5
            w = cnt['bbox'][3]
            d = (W * f) / w

            self.publish_dist_and_pos(x , y, d)

\end{lstlisting}

The \verb|publish_dist_and_pos| function prepares and publishes a \verb|Int32MultiArray| message containing the x and y position and distance of a detected object:

\begin{lstlisting}[language=PythonPlus]
def publish_dist_and_pos(self, x, y, distance):
    msg = Int32MultiArray()
    msg.data = [int(x), int(y), int(distance)]
    self.distance_and_position_publisher.publish(msg)
\end{lstlisting}

The main function initializes the ROS2 client library, creates an instance of our \verb|ObjectDetection| node, waits for incoming messages, properly destroys the node when done, and finally shuts down the ROS2 client library:

\begin{lstlisting}[language=PythonPlus]
def main(args=None):
    rclpy.init(args=args)
    node = ObjectDetection()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
\end{lstlisting}


\subsection{Drone Position Estimation}

In our application, we focus on estimating the drone's position relative to a stationary object. For this, we implement a \verb|DronePositionEstimator| algorithm. This algorithm makes use of yaw angle and the distance between the drone and the object to estimate the position of the drone.

To start with, we import the necessary packages for ROS2 node creation, message handling, drone position calculation, and plotting:

\begin{lstlisting}[language=PythonPlus]
#!/usr/bin/env python3

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu
from std_msgs.msg import Int32MultiArray
from pymavlink import mavutil
from pymavlink_msgs.msg import DronePose
import math
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d
import numpy as np

\end{lstlisting}

Next, we define a class \verb|DronePositionEstimator| that inherits from the ROS2 Node class. The \verb|def __init__| function is for when the drone is first initialized and we use this function to define variables and the creation of subscribers and publishers. This class subscribes to the topics \verb|distance_and_pos| and \verb|drone_pose| that provide the distance to the object and the pose data of the drone, and initializing the variables that we use in the plot.

\begin{lstlisting}[language=PythonPlus]
class FollowAlgorithm(Node):
    def __init__(self): 
        super().__init__('follow_algorithm') 

        self.create_subscription(Int32MultiArray, 'distance_and_pos', self.position_and_distance_callback, 10)
        
        self.create_subscription(DronePose, 'drone_pose', self.qualisys_callback, 10)
        self.distance = 0.0

        #self.fig = plt.axes(projection='3d')
        arrsize = 1000
        self.drone_x_arr = np.zeros(arrsize)
        self.drone_y_arr = np.zeros(arrsize)
        self.drone_z_arr = np.zeros(arrsize)
        self.qualisys_x_arr = np.zeros(arrsize)
        self.qualisys_y_arr = np.zeros(arrsize)
        self.qualisys_z_arr = np.zeros(arrsize)
        self.counter = 0
\end{lstlisting}

In the function \verb|qualisys_callback|, which is triggered when a new message is published on the \verb|drone_pose| topic, the estimated drone position is calculated based on the yaw angle and the distance to the object and then plotting:

\begin{lstlisting}[language=PythonPlus]
def qualisys_callback(self, msg: DronePose):

    object_x = 0.0 # meters
    object_y = 0.0 #meters 
    object_z = 0.0 #meters
    camera_angle = 45  # degrees
    yaw = msg.yaw.data # radians -pi to pi

    true_x = msg.pos.x # x position in meters from qualisys
    true_y = msg.pos.y # y position in meters from qualisys
    true_z = msg.pos.z # z position in meters from qualisys
    
    distance_to_object = self.distance# meters

    # Calculate the drone's position
    drone_x, drone_y, drone_z = self.find_drone_position(yaw, distance_to_object, object_x, object_y, object_z, camera_angle)

    # Plot data
    self.plotter((drone_x, drone_y, drone_z), (true_x, true_y, true_z))
\end{lstlisting}

The function \verb|find_drone_position| is responsible for the calculation of the drone's position:

\begin{lstlisting}[language=PythonPlus]
def find_drone_position(self, yaw, distance_to_object, object_x, object_y, object_z, camera_angle):

    drone_x = object_x - distance_to_object * math.sin(yaw)
    drone_y = object_y + distance_to_object * math.cos(yaw)
    drone_z = 0

    return drone_x, drone_y, drone_z
\end{lstlisting}

The function \verb|position_and_distance_callback| updates the distance when a new message is published on the \verb|distance_and_pos| topic. This function also converts cm into meters and adds the distance from the camera to the center of the drone:

\begin{lstlisting}[language=PythonPlus]
def position_and_distance_callback(self, msg: Int32MultiArray):
    self.distance = (msg.data[2] + 12) / 100
\end{lstlisting}

The function plotter collects and plots the estimated position of the drone and the actual position of the drone, saving the plots when the array is full:

\begin{lstlisting}[language=PythonPlus]
def plotter(self, drone_pos, qualisys_pos):
    if (self.counter < len(self.drone_x_arr)):
        # Scatter plots
        x_drone = drone_pos[0]
        y_drone = drone_pos[1]
        z_drone = drone_pos[2]
        x_qualisys = qualisys_pos[0]
        y_qualisys = qualisys_pos[1]
        z_qualisys = qualisys_pos[2]
        ndx = self.counter
        self.counter += 1

        self.drone_x_arr[ndx] = x_drone
        self.drone_y_arr[ndx] = y_drone
        self.drone_z_arr[ndx] = z_drone
        self.qualisys_x_arr[ndx] = x_qualisys
        self.qualisys_y_arr[ndx] = y_qualisys
        self.qualisys_z_arr[ndx] = 0
    else:
        self.save_plot()
        
def save_plot(self):
    plt.figure()
    fig = plt.axes()
    fig.scatter(self.drone_x_arr, self.drone_y_arr, s=len(self.drone_x_arr), c='Blue', marker='.')
    fig.scatter(self.qualisys_x_arr, self.qualisys_y_arr, s=len(self.qualisys_x_arr), c='Green', marker='.')
    plt.savefig("/home/vaffe/ros/log/plot.png")
\end{lstlisting}

Lastly, in the main function, we initialize a ROS2 node for the \verb|DronePositionEstimator| and spin the node:

\begin{lstlisting}[language=PythonPlus]
def main(args=None):
    rclpy.init(args=args)
    node = DronePositionEstimator()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
\end{lstlisting}

It's important to note that the accuracy of the estimated position depends on the precision of the yaw angle and distance data. 

\subsection{Image Processing, configuration 4 (Code Only)}

\subsubsection{Camera Capture}
\begin{lstlisting}[language=PythonPlus, basicstyle=\tiny,]
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image # import the Image message type
import cv2
from cv_bridge import CvBridge
from picamera2 import Picamera2
from libcamera import controls

class CameraCapture(Node): # inherits from Node
    def __init__(self): # constructor
        super().__init__('camera_capture') # call the constructor of the parent class
        self.publisher_ = self.create_publisher(Image, 'image_data', 10) # create a publisher
        self.timer = self.create_timer(1/30, self.publish_image_data) # create a timer
        self.opencv_video = Picamera2() # open the video capture device
        self.opencv_video.configure(self.opencv_video.create_preview_configuration(main={"format": 'RGB888', "size": (640, 480)}))
        self.opencv_video.set_controls({"AwbEnable": True})
        self.opencv_video.start() 
        self.bridge = CvBridge() # create a bridge between OpenCV and ROS

    def publish_image_data(self): # callback function
        frame = self.opencv_video.capture_array() # read a frame from the video capture device
        #if not success:
       #     self.get_logger().info('Failed to read frame from camera')
       #     return
        msg = self.bridge.cv2_to_imgmsg(frame, encoding='bgr8') # convert the image to a ROS message
        

        self.publisher_.publish(msg) # publish the message

def main(args=None): # args is a list of strings
    rclpy.init(args=args) # initialize the ROS client library

    node = CameraCapture() # create a node

    rclpy.spin(node) # wait for messages
    node.destroy_node() # destroy the node explicitly

    rclpy.shutdown() # shutdown the ROS client library

if __name__ == '__main__': # if this file is run as a script
    main() # run the main function

\end{lstlisting}

\subsubsection{Blob Detection}

\begin{lstlisting}[language=PythonPlus, basicstyle=\tiny,]

#!/usr/bin/etv python

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import Int32MultiArray
import cv2
from cv_bridge import CvBridge
import cvzone
from cvzone.FPS import FPS
from cvzone.ColorModule import ColorFinder
import numpy as np
import time
import math



class ObjectDetection(Node):
    def __init__(self):
        super().__init__('object_detection') # call the constructor of the parent class
        self.subscription = self.create_subscription(Image, 'image_data', self.process_image, 10) # create a subscriber
        self.bridge = CvBridge() # create a bridge between OpenCV and ROS
        self.fpsreader = FPS() # Initialize FPS reader
        self.distance_and_position_publisher = self.create_publisher(Int32MultiArray, 'distance_and_pos', 10)


    def is_circle(self, cnt, threshold=0.75):
            area = cv2.contourArea(cnt)
            perimeter = cv2.arcLength(cnt, True)
            if perimeter == 0:
                return False
            circularity = 4 * np.pi * area / (perimeter * perimeter)
            return circularity >= threshold
    


    def process_image(self, msg): # callback function main processing function

        opencv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
        h, w, _ = opencv_image.shape

        myColorFinder: ColorFinder = ColorFinder(False)
        hsvValsBlue = {'hmin': 104, 'smin': 128, 'vmin': 0, 'hmax': 120, 'smax': 255, 'vmax': 152} #blue
        hsvValsGreen = {'hmin': 26, 'smin': 54, 'vmin': 48, 'hmax': 90, 'smax': 137, 'vmax': 255} #green
        hsvValsRed = {'hmin': 0, 'smin': 120, 'vmin': 120, 'hmax': 20, 'smax': 255, 'vmax': 255} #red

        fps, img = self.fpsreader.update(opencv_image)

        _, maskBlue = myColorFinder.update(img, hsvValsBlue)
        _, maskGreen = myColorFinder.update(img, hsvValsGreen)
        _, maskRed = myColorFinder.update(img, hsvValsRed)


        _, contoursBlue = cvzone.findContours(img, maskBlue)
        _, contoursGreen = cvzone.findContours(img, maskGreen)
        _, contoursRed = cvzone.findContours(img, maskRed)

        # Filter contours that are circles
        circular_contours_blue = [cnt for cnt in contoursBlue if self.is_circle(cnt['cnt'])]
        circular_contours_green = [cnt for cnt in contoursGreen if self.is_circle(cnt['cnt'])]
        circular_contours_red = [cnt for cnt in contoursRed if self.is_circle(cnt['cnt'])]

        # Process and display depth, x, and y position for each ball
        for color, circular_contours_list in zip(['blue', 'green', 'red'],
                                                [circular_contours_blue, circular_contours_green, circular_contours_red]):
            if circular_contours_list:

                cnt = circular_contours_list[0]
                x, y = cnt['center']

                f = 889
                W = 6.5
                w = cnt['bbox'][3]
                d = (W * f) / w
                self.get_logger().info(f"{color}: {d}")
                self.get_logger().info(f" fps: {fps}")
                self.get_logger().info(f" x: {x} y: {y}")

                self.publish_dist_and_pos(x , y, d)

    def publish_dist_and_pos(self, x, y, distance):
        msg = Int32MultiArray()
        msg.data = [int(x), int(y), int(distance)]
        self.distance_and_position_publisher.publish(msg)

def main(args=None): 
    rclpy.init(args=args) 

    node = ObjectDetection()

    rclpy.spin(node)
    node.destroy_node()

    rclpy.shutdown() 

if __name__ == '__main__':
    main()

\end{lstlisting}

\subsubsection{Object Estimation}
\begin{lstlisting}[language=PythonPlus, basicstyle=\tiny,]

#!/usr/bin/env python3

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu
from std_msgs.msg import Int32MultiArray
from pymavlink import mavutil
from pymavlink_msgs.msg import DronePose
import math
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d
import numpy as np


class FollowAlgorithm(Node):
    def __init__(self): 
        super().__init__('follow_algorithm') 

        self.create_subscription(Int32MultiArray, 'distance_and_pos', self.position_and_distance_callback, 10)
        
        self.create_subscription(DronePose, 'drone_pose', self.qualisys_callback, 10)
        self.distance = 0.0

        arrsize = 1000
        self.drone_x_arr = np.zeros(arrsize)
        self.drone_y_arr = np.zeros(arrsize)
        self.drone_z_arr = np.zeros(arrsize)
        self.qualisys_x_arr = np.zeros(arrsize)
        self.qualisys_y_arr = np.zeros(arrsize)
        self.qualisys_z_arr = np.zeros(arrsize)
        self.counter = 0
        
        
    def qualisys_callback(self, msg: DronePose):

        object_x = 0.0  # meters
        object_y = 0.0  # meters
        object_z = 0.0
        camera_angle = 45  # degrees
        yaw = msg.yaw.data # radians -pi to pi
            

        true_x = msg.pos.x # x position in meters from qualisys
        true_y = msg.pos.y # y position in meters from qualisys
        true_z = msg.pos.z
        distance_to_object = self.distance# meters

        drone_x, drone_y, drone_z = self.find_drone_position(yaw, distance_to_object, object_x, object_y, 0, camera_angle)


        self.plotter((drone_x, drone_y, drone_z), (true_x, true_y, true_z))


    def find_drone_position(self, yaw, distance_to_object, object_x, object_y, object_z, camera_angle):
    def find_drone_position(self, yaw, distance_to_object, object_x, object_y, object_z, camera_angle):
    
        # Calculate drone position
        drone_x = object_x - distance_to_object * math.sin(yaw)
        drone_y = object_y + distance_to_object * math.cos(yaw)
        drone_z = 0
        return drone_x, drone_y, drone_z

    def position_and_distance_callback(self, msg: Int32MultiArray):

        self.distance = (msg.data[2] + 12) / 100
        self.get_logger().info(f"Distance to object {self.distance} ")
       

    def plotter(self, drone_pos, qualisys_pos):


        if (self.counter < len(self.drone_x_arr)):
            # Scatter plots
            x_drone = drone_pos[0]
            y_drone = drone_pos[1]
            z_drone = drone_pos[2]
            x_qualisys = qualisys_pos[0]
            y_qualisys = qualisys_pos[1]
            z_qualisys = qualisys_pos[2]
            ndx = self.counter
            self.counter += 1

            self.drone_x_arr[ndx] = x_drone
            self.drone_y_arr[ndx] = y_drone
            self.drone_z_arr[ndx] = z_drone
            self.qualisys_x_arr[ndx] = x_qualisys
            self.qualisys_y_arr[ndx] = y_qualisys
            self.qualisys_z_arr[ndx] = 0
        else:
            self.save_plot()

    def save_plot(self):
        plt.figure()
        fig = plt.axes()
        fig.scatter(self.drone_x_arr, self.drone_y_arr, s=len(self.drone_x_arr), c='Blue', marker='.')
        fig.scatter(self.qualisys_x_arr, self.qualisys_y_arr, s=len(self.qualisys_x_arr), c='Green', marker='.')
        plt.savefig("/home/vaffe/ros/log/plot.png")

def main(args=None):
    rclpy.init(args=args)
    follow_algorithm = FollowAlgorithm()
    rclpy.spin(follow_algorithm)
    follow_algorithm.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()

\end{lstlisting}